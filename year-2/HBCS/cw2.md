As artificial intelligence becomes more of a regular feature in modern society it is important to take a step back and assess the possible consequences of this future development. Artificial general intelligence (AGI) can be classed as a machine or system that is capable of reaching or surpassing human cognitive ability and with the steady advances in the development of this field it is conceivable that this will be reached by the end of the century (CBS, 2023). Signorelli (2018) proposes new ideas on what is needed for a computer to gain human level intelligence, they touch on paradoxes that arise from this idea and ethical concerns that surround creating artificial general intelligence. This essay will dig deeper into these ideas and discuss the moral and ethical consequences that AGI can pose on current and future society, considering the impact on jobs, and how this amplifies societal inequality and cyber security concerns that AI poses on the public.

One of the most prevalent issues with the advancement of AI is the problem of how further automation, especially in traditionally human-centric roles, can have a tremendous impact on the wider job market. This impact has been seen throughout every major technological revolution in history back to the initial industrial revolution in the 1700s when large-scale unemployment due to advancements in technology was documented for the first time in the hand spinning industry (Schneider, 2023). This trend has continued throughout modern history, Frey and Osborne, 2017 studied the potential impacts that automation could have on jobs and found that nearly half of jobs in the US are at risk of being automated. This issue is now more heavily exacerbated if you include AI in these calculations, sectors and skills like fine art, persuasion and the science and technology industry are now at greater risk of damage compared to Osborne and Frey’s study (Bruun and Duka, 2018). There is wide disagreement on AGI job displacement, however. The president of the Information Technology and Innovation Foundation, a US think tank, argues that it is unlikely for AI to have the damaging effects on people’s jobs that many researchers predict and instead just shift the perspective of job roles and complement job roles (Atkinson, 2018). Atkinson may be right in this perspective, however, there needs to be consideration of the difference between previous attempts at automation and this potential AI revolution which could lead to the creation of general intelligence.

As mentioned before, the risk that AGI will take over jobs deemed more ’human’ and the risk of the deskilling of jobs thought to be safe from this (Braverman, 1974), potentially created a sense of purposelessness within society and even civil unrest (Voth and Caprettini, 2017). The fallout of this is widespread socioeconomic damage, the tendency for corporations to prioritise maximising profits over the treatment of their employees (Chomsky, 1998) means if AI shows promise at being as efficient as humans (and in the future think like humans and gain consciousness) the diminished reliance on human workers could widen inequality in society. This issue highlights some of the ethical consequences that arise from creating AGI. A utilitarian view could see how the potential longer-term benefits of AGI presented by Atkinson (2018) could outweigh the initial deep harm it would cause society and continue pushing for the development of this technology. However, the damage may be too catastrophic to recover from. A cautious approach is needed to minimise these risks.

The potential for AGI and AI development to expand societal inequality goes beyond just the general population losing their jobs and being replaced by automation. The growing concentration of power and wealth in society has become a driving factor in the growing disparity between billionaire owners and CEOs and the regular working individual (Chomsky, 2017). This has been a prevalent issue throughout history, but the rise of technology companies has cemented this fact in modern society with trillions being hoarded and hidden away by individual billionaires and corporations (Collins, 2021). This contributing to the stagnation of wages and regression in living standards for the average person, with those on low incomes in the US only seeing an increase of $200 from 2012 to 2019 compared to the 7x that the top elites received in that same period (Piper, 2023). The emergence of AI and the eventual creation of artificial general intelligence look to increase this inequality both in the aspects of job erosion, but also in the idea that the person or company that has control of AGI has wealth, power and influence amplified and creates worse conditions for the many (Brynjolfsson, 2022). This issue is tough to remedy due to the black box problem. The extremely closed-source nature of AI makes it very hard for the public to trust the technology and patent laws that are in place to protect trade secrets, which means that open-source competition is very unlikely to be able to compete with the likes of OpenAI, Google and Microsoft (Eschenbach, 2021).

This vast lack of transparency surrounding how these AI models and algorithms work internally doesn’t just have an impact on wealth inequality but it also has a large impact on societal inequality. Biases that exist within today’s AI models are a result of their training data and often reflect the judgements and social hierarchies that we form as humans, discriminating against protected characteristics like race and gender (Buolamwini and Gebru, 2018; Zajko, 2022). The black box problem causes lots of difficulties when trying to audit these systems, even though policies are being developed to ensure all AI tools are audited for bias (Domin et al., 2022), there are still cases happening now where AI is perpetuating discriminatory bias against data (Yin et al., 2024) and this issue seems to be coming into more focus when looking at the "garbage-in garbage-out" phenomenon first attributed to the IBM engineer George Fuechsel in 1960. GIGO effectively means that if you put useless data into a system you’ll get useless data out, if the original data you feed to an AI system is biased your outputs will be too. This problem could become worse if you consider the massive amount of AI-generated content on the internet now, this data could be used to train new AI models potentially amplifying the existing bias. This is already being reflected in the sentiment that people feel generative AI is becoming less capable and code quality is decreasing (Harding and Kloster, 2024; Chen et al., 2023). The issue extends beyond our AI understanding now and into the realm of AGI, there is no guarantee that an AGI will conform to human ethical and moral standards (Goertzel and Pitt, 2014) and with the recent events surrounding AI bias, it seems very unlikely.

Finally, a significant concern of ever-advancing AI technology lies in the potential security issues that could be exploited and utilised not only by threat actors but by governments against their citizens. Even with our current modest AI systems threat actors are already leveraging this technology to create more convincing phishing emails that bypass email security measures and spam filers, specifically spear-phishing attacks, and even replicate a family member’s voice to scam victims over the phone (Khan et al., 2021; Verma, 2023). NCSC (2024) warns that generative AI will have a major impact on cyber attacks in the next two years, increasing the efficiency at which threat actors can attack targets. The creation of AGI will bring these issues into greater view. If an AI reaches  
or surpasses human intelligence and is trained to perform complex cyber-attacks, the inevitable elimination of  
human interaction means cyber attacks could happen more frequently and more efficiently (Haney, 2018; Gün-  
do ̆gar and Niauronis, 2023)